# Q-Learning & DQN (Deep Q-Network)

> Value-Based ê°•í™”í•™ìŠµì˜ í•µì‹¬: Q-Learningì—ì„œ DQNê¹Œì§€

---

## ğŸ“š ëª©ì°¨

1. [Q-Learning ê¸°ë³¸ ê°œë…](#-q-learning-ê¸°ë³¸-ê°œë…)
2. [Q-Learningì˜ í•œê³„ì ](#-q-learningì˜-í•œê³„ì )
3. [DQNì˜ ë“±ì¥](#-dqnì˜-ë“±ì¥)
4. [Q-Learning vs DQN ë¹„êµ](#-q-learning-vs-dqn-ë¹„êµ)
5. [ì‹¤ìŠµ ì½”ë“œ](#-ì‹¤ìŠµ-ì½”ë“œ)

---

## ğŸ¯ Q-Learning ê¸°ë³¸ ê°œë…

### ì •ì˜

**Q-Learning**ì€ **Model-Free**, **Off-Policy**, **Value-Based** ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.

- **Model-Free**: í™˜ê²½ì˜ ëª¨ë¸(ì „ì´ í™•ë¥ , ë³´ìƒ í•¨ìˆ˜)ì„ ëª°ë¼ë„ í•™ìŠµ ê°€ëŠ¥
- **Off-Policy**: í–‰ë™í•˜ëŠ” ì •ì±…ê³¼ í•™ìŠµí•˜ëŠ” ì •ì±…ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
- **Value-Based**: Q-Function(Action-Value Function)ì„ í•™ìŠµ

### Q-Tableì´ë€?

**Q-Table (Q-í…Œì´ë¸”)** ì€ ëª¨ë“  **(ìƒíƒœ, í–‰ë™) ìŒì— ëŒ€í•œ Qê°’(ê¸°ëŒ“ê°’)** ì„ ì €ì¥í•˜ëŠ” í‘œì…ë‹ˆë‹¤.

**êµ¬ì¡°:**
- **í–‰(Row)**: ê° ìƒíƒœ (State)
- **ì—´(Column)**: ê° í–‰ë™ (Action)  
- **ì…€ ê°’**: Q(s, a) = ìƒíƒœ sì—ì„œ í–‰ë™ aë¥¼ í–ˆì„ ë•Œì˜ **ê¸°ëŒ€ ëˆ„ì  reward**

**ìˆ˜ì‹:**
$$Q(s, a) = \mathbb{E}[R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \cdots \mid S_t = s, A_t = a]$$

**ì˜ë¯¸:**
- ìƒíƒœ sì—ì„œ í–‰ë™ aë¥¼ ì„ íƒí•œ í›„, ê³„ì† ì •ì±…ì„ ë”°ëì„ ë•Œ ë°›ì„ **ì´ ëˆ„ì  ë³´ìƒì˜ ê¸°ëŒ“ê°’**

**ì˜ˆì‹œ:**
```
Q(ìƒíƒœ=3, í–‰ë™=ì˜¤ë¥¸ìª½) = 0.8
â†’ "ìƒíƒœ 3ì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ê°€ë©´ ì•ìœ¼ë¡œ í‰ê·  0.8ì˜ ë³´ìƒì„ ë°›ì„ ê²ƒ"
```

### í•µì‹¬ ì•„ì´ë””ì–´

**"ëª¨ë“  (ìƒíƒœ, í–‰ë™) ìŒì˜ ê°€ì¹˜ë¥¼ í…Œì´ë¸”ì— ì €ì¥í•˜ê³ , ê²½í—˜ì„ í†µí•´ ì—…ë°ì´íŠ¸í•œë‹¤"**

```
Q-Table:
         Action 0  Action 1  Action 2  Action 3
State 0    0.5       0.3       0.7       0.2
State 1    0.8       0.6       0.4       0.9
State 2    0.3       0.7       0.5       0.6
...
```

### Q-Learning ì•Œê³ ë¦¬ì¦˜

**1. Q-Table ì´ˆê¸°í™”**
```python
Q(s,a) = 0  # ëª¨ë“  ìƒíƒœ-í–‰ë™ ìŒì— ëŒ€í•´
```

**2. ì—í”¼ì†Œë“œ ë°˜ë³µ:**

```python
for episode in range(num_episodes):
    state = env.reset()
    
    while not done:
        # (1) Îµ-greedyë¡œ í–‰ë™ ì„ íƒ
        if random() < epsilon:
            action = random_action()  # íƒìƒ‰ (Exploration)
        else:
            action = argmax(Q[state])  # í™œìš© (Exploitation)
        
        # (2) í–‰ë™ ì‹¤í–‰
        next_state, reward, done = env.step(action)
        
        # (3) Q-Table ì—…ë°ì´íŠ¸ (í•µì‹¬!)
        Q[state, action] = Q[state, action] + Î± * [
            reward + Î³ * max(Q[next_state]) - Q[state, action]
        ]
        
        state = next_state
```

### Bellman Optimality Equation ê¸°ë°˜ ì—…ë°ì´íŠ¸

$$Q(s,a) \leftarrow Q(s,a) + \alpha \left[ \underbrace{r + \gamma \max_{a'} Q(s',a')}_{\text{TD Target}} - Q(s,a) \right]$$

**ê° í•­ì˜ ì˜ë¯¸:**
- $Q(s,a)$ : í˜„ì¬ Qê°’ (ì—…ë°ì´íŠ¸ ëŒ€ìƒ)
- $\alpha$ : í•™ìŠµë¥  (Learning Rate, 0 < Î± â‰¤ 1)
- $r$ : ì¦‰ê°ì  ë³´ìƒ (Immediate Reward)
- $\gamma$ : í• ì¸ìœ¨ (Discount Factor, 0 â‰¤ Î³ < 1)
- $\max_{a'} Q(s',a')$ : ë‹¤ìŒ ìƒíƒœì—ì„œì˜ ìµœëŒ€ Qê°’
- $\left[ \cdots \right]$ : **TD Error** (Temporal Difference Error)

**TD Errorì˜ ì˜ë¯¸:**
- TD Target - í˜„ì¬ ì¶”ì •ì¹˜
- "ì˜ˆìƒë³´ë‹¤ ì¢‹ì•˜ìœ¼ë©´" â†’ ì–‘ìˆ˜ â†’ Qê°’ ì¦ê°€
- "ì˜ˆìƒë³´ë‹¤ ë‚˜ë¹´ìœ¼ë©´" â†’ ìŒìˆ˜ â†’ Qê°’ ê°ì†Œ

### Q-Learningì˜ íŠ¹ì§•

#### âœ… ì¥ì 

1. **êµ¬í˜„ì´ ê°„ë‹¨í•¨**
   - Q-Tableë§Œ ìˆìœ¼ë©´ ë¨
   - ì´í•´í•˜ê¸° ì‰¬ìš´ ì•Œê³ ë¦¬ì¦˜

2. **Model-Free**
   - í™˜ê²½ì˜ ì „ì´ í™•ë¥ ì„ ëª°ë¼ë„ í•™ìŠµ ê°€ëŠ¥
   - ì‹¤ì œ ê²½í—˜ë§Œìœ¼ë¡œ í•™ìŠµ

3. **Off-Policy**
   - íƒìƒ‰ê³¼ í™œìš©ì„ ë¶„ë¦¬ ê°€ëŠ¥
   - ë‹¤ë¥¸ ì •ì±…ì˜ ê²½í—˜ìœ¼ë¡œë„ í•™ìŠµ ê°€ëŠ¥

4. **ìˆ˜ë ´ ë³´ì¥**
   - ì¶©ë¶„í•œ íƒìƒ‰ê³¼ ì ì ˆí•œ í•™ìŠµë¥  ì¡°ê±´ í•˜ì—
   - ìµœì  Q-Functionìœ¼ë¡œ ìˆ˜ë ´ ë³´ì¥

#### ğŸ“Š ë™ì‘ ì˜ˆì‹œ

**Grid World ì˜ˆì‹œ:**
```
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚  S  â”‚     â”‚     â”‚  G  â”‚  S: Start, G: Goal
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚     â”‚  X  â”‚     â”‚     â”‚  X: ì¥ì• ë¬¼
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚     â”‚     â”‚     â”‚     â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜

Actions: â†‘(0), â†“(1), â†(2), â†’(3)
Rewards: Goal=+1, ì¥ì• ë¬¼=-1, ë‚˜ë¨¸ì§€=0
```

**í•™ìŠµ ê³¼ì •:**
```
Episode 1: S â†’ â†’ â†‘ X (-1 ë³´ìƒ) â†’ Q-Table ì—…ë°ì´íŠ¸
Episode 2: S â†’ â†“ â†’ â†’ G (+1 ë³´ìƒ) â†’ Q-Table ì—…ë°ì´íŠ¸
Episode 3: S â†’ â†’ â†’ G (+1 ë³´ìƒ) â†’ Q-Table ì—…ë°ì´íŠ¸
...
Episode 1000: ìµœì  ê²½ë¡œ í•™ìŠµ ì™„ë£Œ!
```

---

## âš ï¸ Q-Learningì˜ í•œê³„ì 

### 1ï¸âƒ£ **ìƒíƒœ ê³µê°„ í­ë°œ (State Space Explosion)**

**ë¬¸ì œ:**
- Q-Tableì€ ëª¨ë“  (ìƒíƒœ, í–‰ë™) ìŒì„ ì €ì¥í•´ì•¼ í•¨
- ìƒíƒœê°€ ë§ì•„ì§€ë©´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¦ê°€

**ì˜ˆì‹œ:**

| í™˜ê²½ | ìƒíƒœ ìˆ˜ | í–‰ë™ ìˆ˜ | Q-Table í¬ê¸° |
|------|---------|---------|--------------|
| Grid World (4Ã—4) | 16 | 4 | 64 |
| Atari Pong (84Ã—84 grayscale) | 256^(84Ã—84) â‰ˆ 10^14000 | 6 | ğŸ’¥ ë¶ˆê°€ëŠ¥ |
| ë°”ë‘‘ (19Ã—19) | 3^361 â‰ˆ 10^172 | 361 | ğŸ’¥ ë¶ˆê°€ëŠ¥ |

```python
# Grid WorldëŠ” ê´œì°®ìŒ
Q_table = np.zeros((16, 4))  # 64ê°œ ì—”íŠ¸ë¦¬

# AtariëŠ”?
Q_table = np.zeros((256^7056, 6))  # ğŸ˜± ë©”ëª¨ë¦¬ ë¶€ì¡±!
```

### 2ï¸âƒ£ **ì—°ì† ìƒíƒœ ê³µê°„ ì²˜ë¦¬ ë¶ˆê°€**

**ë¬¸ì œ:**
- Q-Tableì€ ì´ì‚°(discrete) ìƒíƒœë§Œ ë‹¤ë£° ìˆ˜ ìˆìŒ
- ì—°ì†(continuous) ìƒíƒœëŠ” ë¬´í•œê°œì´ë¯€ë¡œ í…Œì´ë¸”ë¡œ í‘œí˜„ ë¶ˆê°€ëŠ¥

**ì˜ˆì‹œ:**
```python
# ë¡œë´‡ íŒ” ì œì–´
state = [
    joint_angle_1,      # 0.0 ~ 360.0ë„ (ì—°ì†)
    joint_angle_2,      # 0.0 ~ 360.0ë„ (ì—°ì†)
    joint_velocity_1,   # -âˆ ~ +âˆ (ì—°ì†)
    joint_velocity_2,   # -âˆ ~ +âˆ (ì—°ì†)
]

# ì´ê±¸ Q-Tableë¡œ? ğŸ˜±
# â†’ ë¬´í•œí•œ ì¡°í•©, ë©”ëª¨ë¦¬ì— ì €ì¥ ë¶ˆê°€ëŠ¥
```

**ì„ì‹œ í•´ê²°ì±…: Discretization (ì´ì‚°í™”)**
```python
# ê°ë„ë¥¼ 10ë„ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê¸°
joint_angle_1 = round(angle / 10) * 10  # 0, 10, 20, ..., 360

# ë¬¸ì œ:
# - ì •ë°€ë„ ì†ì‹¤ (30.5ë„ â†’ 30ë„ë¡œ ê·¼ì‚¬)
# - ì—¬ì „íˆ ì¡°í•© í­ë°œ ê°€ëŠ¥
```

### 3ï¸âƒ£ **ì¼ë°˜í™” ëŠ¥ë ¥ ë¶€ì¡± (No Generalization)**

**ë¬¸ì œ:**
- ë¹„ìŠ·í•œ ìƒíƒœë¥¼ ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµ
- ê²½í—˜ì„ ê³µìœ í•˜ì§€ ëª»í•¨

**ì˜ˆì‹œ:**
```python
# Grid Worldì—ì„œ
state_1 = (3, 4)  # Qê°’ í•™ìŠµë¨
state_2 = (3, 5)  # ë¹„ìŠ·í•œ ìƒíƒœì¸ë° ì²˜ìŒë¶€í„° í•™ìŠµí•´ì•¼ í•¨

# ì‚¬ëŒì€ ì•Œ ìˆ˜ ìˆìŒ:
# "ì•„, (3,4)ë‘ ë¹„ìŠ·í•˜ë„¤? ë¹„ìŠ·í•˜ê²Œ í–‰ë™í•˜ë©´ ë˜ê² êµ¬ë‚˜!"
# í•˜ì§€ë§Œ Q-Learningì€ ì´ê±¸ ëª¨ë¦„ ğŸ˜­
```

**ì´ë¯¸ì§€ ì˜ˆì‹œ:**
```
Atari Pongì—ì„œ:
í”„ë ˆì„ t:   ê³µì´ ì™¼ìª½ ìœ„ì— ìˆìŒ â†’ Qê°’ í•™ìŠµ
í”„ë ˆì„ t+1: ê³µì´ ì™¼ìª½ ìœ„ì—ì„œ 1í”½ì…€ ì›€ì§ì„
            â†’ ì™„ì „íˆ ë‹¤ë¥¸ ìƒíƒœë¡œ ì·¨ê¸‰!
            â†’ ì²˜ìŒë¶€í„° ë‹¤ì‹œ í•™ìŠµ ğŸ˜±
```

### 4ï¸âƒ£ **ìƒ˜í”Œ íš¨ìœ¨ì„± ë‚®ìŒ**

**ë¬¸ì œ:**
- ëª¨ë“  ìƒíƒœë¥¼ ì¶©ë¶„íˆ ë°©ë¬¸í•´ì•¼ í•¨
- í•™ìŠµì— ë§ì€ ì—í”¼ì†Œë“œ í•„ìš”

**ì˜ˆì‹œ:**
```python
# 16ê°œ ìƒíƒœ Ã— 4ê°œ í–‰ë™ = 64ê°œ Qê°’
# ê°ê° ì¶©ë¶„íˆ ì—…ë°ì´íŠ¸í•˜ë ¤ë©´?
# â†’ ìˆ˜ì²œ ~ ìˆ˜ë§Œ ì—í”¼ì†Œë“œ í•„ìš”

# Atarië¼ë©´?
# â†’ 10^14000ê°œ ìƒíƒœ-í–‰ë™ ìŒ... 
# â†’ ìš°ì£¼ì˜ ë‚˜ì´ë³´ë‹¤ ì˜¤ë˜ ê±¸ë¦¼ ğŸ’€
```

### 5ï¸âƒ£ **ê³ ì°¨ì› ì…ë ¥ ì²˜ë¦¬ ë¶ˆê°€**

**ë¬¸ì œ:**
- ì´ë¯¸ì§€, ì„¼ì„œ ë°ì´í„° ë“± ë³µì¡í•œ ì…ë ¥ì„ ì§ì ‘ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ
- Feature Engineeringì´ í•„ìš”

**ì˜ˆì‹œ:**
```python
# Atari ê²Œì„ í™”ë©´: 84Ã—84Ã—3 RGB ì´ë¯¸ì§€
# Q-Learning: ì´ê±¸ ì–´ë–»ê²Œ ìƒíƒœë¡œ? ğŸ¤”

# ë°©ë²• 1: í”½ì…€ ìì²´ë¥¼ ìƒíƒœë¡œ?
state = image  # 256^(84Ã—84Ã—3) ê°€ì§€ ìƒíƒœ â†’ ë¶ˆê°€ëŠ¥

# ë°©ë²• 2: ìˆ˜ë™ìœ¼ë¡œ íŠ¹ì§• ì¶”ì¶œ?
state = [ball_x, ball_y, paddle_x, paddle_y]  # í˜ë“¦, í•œê³„ ìˆìŒ
```

---

## ğŸš€ DQNì˜ ë“±ì¥

### "Deep Q-Network: ë”¥ëŸ¬ë‹ + Q-Learning"

**í•µì‹¬ ì•„ì´ë””ì–´:**
> **Q-Tableì„ ì‹ ê²½ë§(Neural Network)ìœ¼ë¡œ ëŒ€ì²´í•˜ì!**

```
Q-Learning:  Q-Table[state, action] â†’ Qê°’
                      â†“
DQN:        Neural Network(state) â†’ [Q(s,aâ‚€), Q(s,aâ‚), ..., Q(s,aâ‚™)]
```

### DQNì˜ ê¸°ë³¸ êµ¬ì¡°

#### 1. Q-Network (ì‹ ê²½ë§)

**ì…ë ¥:** ìƒíƒœ (State)  
**ì¶œë ¥:** ëª¨ë“  í–‰ë™ì˜ Qê°’

```python
class DQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_dim, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, action_dim)
    
    def forward(self, state):
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        q_values = self.fc3(x)  # [Q(s,aâ‚€), Q(s,aâ‚), ..., Q(s,aâ‚™)]
        return q_values
```

**ì˜ˆì‹œ:**
```python
state = [0.5, 0.3, 0.8, 0.2]  # 4ì°¨ì› ìƒíƒœ
q_values = dqn(state)         # [0.7, 0.3, 0.9, 0.4]
                              # â†‘ ê° í–‰ë™ì˜ Qê°’
action = argmax(q_values)     # action = 2 (Qê°’ì´ 0.9ë¡œ ìµœëŒ€)
```

#### 2. Loss Function

**ëª©í‘œ:** Neural Networkê°€ Bellman Equationì„ ë§Œì¡±í•˜ë„ë¡ í•™ìŠµ

$$\text{Loss} = \mathbb{E}\left[ \left( \underbrace{r + \gamma \max_{a'} Q(s', a'; \theta^-)}_{\text{TD Target}} - \underbrace{Q(s, a; \theta)}_{\text{Prediction}} \right)^2 \right]$$

**ì˜ë¯¸:**
- TD Target: "ì •ë‹µ"ì— í•´ë‹¹ (Bellman Equation)
- Prediction: í˜„ì¬ ë„¤íŠ¸ì›Œí¬ì˜ ì˜ˆì¸¡
- Loss: ë‘˜ì˜ ì°¨ì´ë¥¼ ìµœì†Œí™”

```python
# TD Target ê³„ì‚°
with torch.no_grad():
    next_q_values = target_net(next_state)
    td_target = reward + gamma * torch.max(next_q_values)

# Current Qê°’
current_q = q_net(state)[action]

# Loss
loss = F.mse_loss(current_q, td_target)
```

### DQNì˜ í˜ì‹ ì  ê¸°ë²•

#### ğŸ§  Experience Replay (ê²½í—˜ ì¬ìƒ)

1ï¸âƒ£ **ë¬¸ì œ (Problem)**
- ì—ì´ì „íŠ¸ê°€ í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©í•˜ë©° ì–»ëŠ” ê²½í—˜ì€ **ì‹œê°„ì ìœ¼ë¡œ ì—°ì†ë˜ì–´ ìƒê´€ê´€ê³„ê°€ ë†’ìŒ**
- ë°ì´í„°ë¥¼ ìˆœì„œëŒ€ë¡œ í•™ìŠµí•˜ë©´ **ê³¼ì í•©(Overfitting)** ë° **í•™ìŠµ ë¶ˆì•ˆì •ì„±(Unstable Learning)** ë°œìƒ
- ê³¼ê±° ê²½í—˜(**í¬ê·€í•œ ê²½í—˜**)ì„ í•œ ë²ˆë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ **ë°ì´í„° íš¨ìœ¨(Data Efficiency)** ì´ ë‚®ìŒ

---

2ï¸âƒ£ **í•´ê²°ì±… (Solution)**
- **Replay Buffer**(ë˜ëŠ” Experience Replay)ì— ê²½í—˜ì„ ì €ì¥í•˜ì—¬, ê³¼ê±° ë°ì´í„°ë¥¼ ì¬ì‚¬ìš©  
    $(s_t, a_t, r_t, s_{t+1}, done)$
- í•™ìŠµ ì‹œ, ì €ì¥ëœ ê²½í—˜ì„ **ë¬´ì‘ìœ„(mini-batch)** ë¡œ ìƒ˜í”Œë§ â†’ **ìƒê´€ê´€ê³„(correlation)** ê°ì†Œ  
- ì´ë¡œì¨ **ì•ˆì •ì ì´ê³  íš¨ìœ¨ì ì¸ í•™ìŠµ** ê°€ëŠ¥



```python
import random
from collections import deque

class ReplayBuffer:
    def __init__(self, capacity=10000):
        # ê²½í—˜ì„ ì €ì¥í•  ë²„í¼ (ìµœëŒ€ í¬ê¸° ì§€ì •)
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        # í•˜ë‚˜ì˜ ê²½í—˜ì„ ë²„í¼ì— ì¶”ê°€
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size=32):
        # ë¬´ì‘ìœ„ë¡œ ê²½í—˜ ìƒ˜í”Œë§ â†’ ì‹œê°„ì  ìƒê´€ê´€ê³„ ì œê±°
        return random.sample(self.buffer, batch_size)
    
    def __len__(self):
        return len(self.buffer)
```

**ì¥ì :**
- âœ… ìƒ˜í”Œ íš¨ìœ¨ì„± í–¥ìƒ (Sample Efficiency) â€” ê³¼ê±° ê²½í—˜ì„ ì¬ì‚¬ìš© ê°€ëŠ¥
- âœ… ì‹œê°„ì  ìƒê´€ì„± ì œê±° (Decorrelation) â€” ì•ˆì •ì  í•™ìŠµ ê°€ëŠ¥
- âœ… Off-policy í•™ìŠµ ì§€ì› â€” ê³¼ê±° ì •ì±…ìœ¼ë¡œ ìˆ˜ì§‘ëœ ë°ì´í„°ë„ í•™ìŠµì— í™œìš© ê°€ëŠ¥
- âœ… Offline Learning ê°€ëŠ¥ â€” í™˜ê²½ ì—†ì´ ì €ì¥ëœ ê²½í—˜ìœ¼ë¡œë„ í•™ìŠµ ê°€ëŠ¥

#### 2ï¸âƒ£ **Target Network (íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬)**

**ë¬¸ì œ:**
- TD Targetì„ ê³„ì‚°í•  ë•Œ ê°™ì€ ë„¤íŠ¸ì›Œí¬ ì‚¬ìš©
- Targetì´ ê³„ì† ì›€ì§ì„ â†’ "Moving Target Problem"
- í•™ìŠµ ë¶ˆì•ˆì • (ë°œì‚° ê°€ëŠ¥)

```python
# ë¬¸ì œê°€ ë˜ëŠ” ê²½ìš°:
td_target = reward + gamma * max(Q_net(next_state))  # Q_netì´ ê³„ì† ë³€í•¨
loss = (td_target - Q_net(state)[action])^2          # ëª©í‘œê°€ ì›€ì§ì„!
```

**í•´ê²°ì±…:**
- **Target Network** ì‚¬ìš© (ê°€ì¤‘ì¹˜ ê³ ì •)
- ì¼ì • ì£¼ê¸°ë§ˆë‹¤ë§Œ ì—…ë°ì´íŠ¸

```python
# Q-Network (ê³„ì† í•™ìŠµë¨)
q_net = DQN(state_dim, action_dim)

# Target Network (ê°€ë” ì—…ë°ì´íŠ¸)
target_net = DQN(state_dim, action_dim)
target_net.load_state_dict(q_net.state_dict())  # ë³µì‚¬

# í•™ìŠµ
for step in range(num_steps):
    # TD Targetì€ Target Networkë¡œ ê³„ì‚° (ê³ ì •ëœ ëª©í‘œ)
    td_target = reward + gamma * max(target_net(next_state))
    
    # Q-Networkë§Œ ì—…ë°ì´íŠ¸
    loss = (td_target - q_net(state)[action])^2
    loss.backward()
    optimizer.step()
    
    # C ìŠ¤í…ë§ˆë‹¤ Target Network ì—…ë°ì´íŠ¸
    if step % C == 0:
        target_net.load_state_dict(q_net.state_dict())
```

**ì¥ì :**
- âœ… í•™ìŠµ ì•ˆì •í™” (ê³ ì •ëœ ëª©í‘œ)
- âœ… ë°œì‚° ë°©ì§€

#### 3ï¸âƒ£ **Convolutional Neural Network (CNN)**

**ì´ë¯¸ì§€ ì…ë ¥ ì²˜ë¦¬:**
- Atari ê²Œì„ì²˜ëŸ¼ ê³ ì°¨ì› ì´ë¯¸ì§€ ì…ë ¥
- CNNìœ¼ë¡œ íŠ¹ì§• ìë™ ì¶”ì¶œ

```python
class DQN_CNN(nn.Module):
    def __init__(self, action_dim):
        super(DQN_CNN, self).__init__()
        # ì…ë ¥: 84Ã—84Ã—4 (4í”„ë ˆì„ stacked)
        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)
        
        self.fc1 = nn.Linear(7 * 7 * 64, 512)
        self.fc2 = nn.Linear(512, action_dim)
    
    def forward(self, state):
        x = F.relu(self.conv1(state))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = x.view(x.size(0), -1)  # Flatten
        x = F.relu(self.fc1(x))
        q_values = self.fc2(x)
        return q_values
```

### DQN ì•Œê³ ë¦¬ì¦˜ ì „ì²´ íë¦„

```python
# 1. ì´ˆê¸°í™”
q_net = DQN(state_dim, action_dim)
target_net = DQN(state_dim, action_dim)
target_net.load_state_dict(q_net.state_dict())
replay_buffer = ReplayBuffer(capacity=10000)

# 2. í•™ìŠµ ë£¨í”„
for episode in range(num_episodes):
    state = env.reset()
    
    for t in range(max_steps):
        # (1) Îµ-greedyë¡œ í–‰ë™ ì„ íƒ
        if random() < epsilon:
            action = env.action_space.sample()
        else:
            q_values = q_net(state)
            action = torch.argmax(q_values)
        
        # (2) í–‰ë™ ì‹¤í–‰
        next_state, reward, done, _ = env.step(action)
        
        # (3) ê²½í—˜ ì €ì¥ (Experience Replay)
        replay_buffer.push(state, action, reward, next_state, done)
        
        # (4) ë¯¸ë‹ˆë°°ì¹˜ ìƒ˜í”Œë§ & í•™ìŠµ
        if len(replay_buffer) > batch_size:
            batch = replay_buffer.sample(batch_size)
            
            # TD Target ê³„ì‚° (Target Network)
            with torch.no_grad():
                next_q = target_net(next_states)
                td_target = rewards + gamma * torch.max(next_q, dim=1)[0]
            
            # Current Qê°’
            current_q = q_net(states).gather(1, actions)
            
            # Loss & ì—…ë°ì´íŠ¸
            loss = F.mse_loss(current_q, td_target)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        
        # (5) Target Network ì—…ë°ì´íŠ¸
        if t % target_update_freq == 0:
            target_net.load_state_dict(q_net.state_dict())
        
        state = next_state
        if done:
            break
```

---

## ğŸ“Š Q-Learning vs DQN ë¹„êµ

### ì¢…í•© ë¹„êµí‘œ

| íŠ¹ì§• | Q-Learning | DQN |
|------|-----------|-----|
| **Qê°’ ì €ì¥ ë°©ì‹** | Q-Table (ë”•ì…”ë„ˆë¦¬/ë°°ì—´) | Neural Network |
| **ìƒíƒœ ê³µê°„** | ì´ì‚°, ì‘ì€ ê³µê°„ë§Œ ê°€ëŠ¥ | ì—°ì†, ê³ ì°¨ì› ê°€ëŠ¥ |
| **ë©”ëª¨ë¦¬ ì‚¬ìš©** | ìƒíƒœ ìˆ˜ì— ë¹„ë¡€ (ê¸°í•˜ê¸‰ìˆ˜) | ë„¤íŠ¸ì›Œí¬ í¬ê¸° (ê³ ì •) |
| **ì¼ë°˜í™” ëŠ¥ë ¥** | âŒ ì—†ìŒ (ê° ìƒíƒœ ë…ë¦½) | âœ… ìˆìŒ (ë¹„ìŠ·í•œ ìƒíƒœ ê³µìœ ) |
| **ì…ë ¥ íƒ€ì…** | ìˆ«ì (ì´ì‚° ìƒíƒœ) | ì´ë¯¸ì§€, ì„¼ì„œ ë°ì´í„° ë“± |
| **í•™ìŠµ ì•ˆì •ì„±** | âœ… ì•ˆì •ì  | âš ï¸ ë¶ˆì•ˆì • (ê¸°ë²• í•„ìš”) |
| **ìƒ˜í”Œ íš¨ìœ¨ì„±** | ë‚®ìŒ | ì¤‘ê°„ (Replay Buffer) |
| **ìˆ˜ë ´ ë³´ì¥** | âœ… ì´ë¡ ì  ë³´ì¥ | âš ï¸ ë³´ì¥ ì—†ìŒ (ì‹¤í—˜ì ) |
| **êµ¬í˜„ ë‚œì´ë„** | ì‰¬ì›€ â­ | ì–´ë ¤ì›€ â­â­â­ |
| **ê³„ì‚° ë¹„ìš©** | ë‚®ìŒ | ë†’ìŒ (GPU í•„ìš”) |
| **ëŒ€í‘œ ì ìš©** | Toy Problems | Atari, ë¡œë´‡ ì œì–´ |

### ìƒì„¸ ë¹„êµ

#### 1. **ìƒíƒœ í‘œí˜„ (State Representation)**

**Q-Learning:**
```python
# ì´ì‚° ìƒíƒœë§Œ ê°€ëŠ¥
state = 5  # ìƒíƒœ ë²ˆí˜¸
Q_table[5][2]  # ìƒíƒœ 5ì—ì„œ í–‰ë™ 2ì˜ Qê°’
```

**DQN:**
```python
# ì—°ì†, ê³ ì°¨ì› ëª¨ë‘ ê°€ëŠ¥
state = np.array([0.5, 0.3, 0.8])  # ì—°ì† ê°’
state = image  # 84Ã—84Ã—3 ì´ë¯¸ì§€
q_values = dqn_net(state)  # Neural Networkë¡œ ì²˜ë¦¬
```

#### 2. **í•™ìŠµ ë°©ì‹**

**Q-Learning:**
```python
# ì§ì ‘ ì—…ë°ì´íŠ¸ (í…Œì´ë¸” ê°’ ë³€ê²½)
Q[s, a] = Q[s, a] + Î± * (td_target - Q[s, a])
```

**DQN:**
```python
# Gradient Descentë¡œ í•™ìŠµ
loss = (td_target - q_net(s)[a])^2
optimizer.zero_grad()
loss.backward()
optimizer.step()
```

#### 3. **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**

**Q-Learning:**
```python
# Grid World 10Ã—10, 4 actions
Q_table = np.zeros((100, 4))  # 400 floats = 1.6 KB

# Atari (ë¶ˆê°€ëŠ¥)
Q_table = np.zeros((256^7056, 6))  # ğŸ’¥ ë©”ëª¨ë¦¬ ë¶€ì¡±
```

**DQN:**
```python
# ìƒíƒœ í¬ê¸°ì™€ ë¬´ê´€í•˜ê²Œ ë„¤íŠ¸ì›Œí¬ í¬ê¸°ë§Œ í•„ìš”
dqn_net = DQN(state_dim, action_dim)
# íŒŒë¼ë¯¸í„° ìˆ˜: ìˆ˜ë§Œ ~ ìˆ˜ë°±ë§Œ ê°œ (ê³ ì •)
# ë©”ëª¨ë¦¬: ~10MB (GPU VRAM í¬í•¨)
```

#### 4. **ì¼ë°˜í™” ëŠ¥ë ¥**

**Q-Learning:**
```python
# ìƒíƒœ (3, 4)ë¥¼ í•™ìŠµí–ˆì–´ë„
Q[3, 4][0] = 0.8  # í•™ìŠµë¨

# ìƒíƒœ (3, 5)ëŠ” ì²˜ìŒë¶€í„°
Q[3, 5][0] = 0.0  # ì´ˆê¸°ê°’ ê·¸ëŒ€ë¡œ
```

**DQN:**
```python
# ë¹„ìŠ·í•œ ìƒíƒœëŠ” ë¹„ìŠ·í•œ Qê°’
state_1 = [3, 4, ...]
state_2 = [3, 5, ...]  # ë¹„ìŠ·í•œ ì…ë ¥

q_net(state_1)  # [0.8, 0.3, ...]
q_net(state_2)  # [0.75, 0.35, ...]  # ë¹„ìŠ·í•œ ì¶œë ¥!
```




---

## ğŸ’¡ í•µì‹¬ ìš”ì•½

### Q-Learning â†’ DQN ì§„í™” ê³¼ì •

```
Q-Learningì˜ í•œê³„:
â”œâ”€ âŒ ìƒíƒœ ê³µê°„ í­ë°œ
â”œâ”€ âŒ ì—°ì† ìƒíƒœ ì²˜ë¦¬ ë¶ˆê°€
â”œâ”€ âŒ ì¼ë°˜í™” ëŠ¥ë ¥ ë¶€ì¡±
â””â”€ âŒ ê³ ì°¨ì› ì…ë ¥ ì²˜ë¦¬ ë¶ˆê°€
         â†“
    "ì‹ ê²½ë§ìœ¼ë¡œ í•´ê²°í•˜ì!"
         â†“
DQNì˜ í˜ì‹ :
â”œâ”€ âœ… Q-Table â†’ Neural Network
â”œâ”€ âœ… Experience Replay
â”œâ”€ âœ… Target Network
â””â”€ âœ… CNN (ì´ë¯¸ì§€ ì²˜ë¦¬)
```

### í•µì‹¬ ë©”ì‹œì§€

> **Q-Learningì€ ê°•í™”í•™ìŠµì˜ ê¸°ì´ˆì´ì ë³¸ì§ˆì„ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.**  
> **DQNì€ Q-Learningì„ ì‹¤ì œ ë³µì¡í•œ ë¬¸ì œì— ì ìš© ê°€ëŠ¥í•˜ê²Œ ë§Œë“  í˜ì‹ ì…ë‹ˆë‹¤.**

---

## ğŸ”— ì‹¤ìŠµ ì½”ë“œ

ì´ í´ë”ì˜ Jupyter Notebook íŒŒì¼ë“¤ì„ ì°¸ê³ í•˜ì„¸ìš”:

- `1.Q-Learning.ipynb`: Q-Learning ê¸°ì´ˆ êµ¬í˜„
- `2.DQN.ipynb`: DQN êµ¬í˜„ ë° Atari ì ìš©

---


**ì‘ì„±ì¼:** 2025-10-23
**ì‘ì„±ì:** Taein Yong


