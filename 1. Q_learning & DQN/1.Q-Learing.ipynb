{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dfa2acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습된 Q-Table:\n",
      "[[0.48767498 0.4868947  0.48377358 0.39013998]\n",
      " [0.51334208 0.51334057 0.51330923 0.5131053 ]\n",
      " [0.54036009 0.54035981 0.51874568 0.54035981]\n",
      " [0.56880009 0.56880009 0.56880009 0.56880009]\n",
      " [0.59873615 0.59873694 0.59873694 0.59873694]\n",
      " [0.63024941 0.63024941 0.63024941 0.63024941]\n",
      " [0.66342043 0.66342043 0.66342043 0.66342043]\n",
      " [0.6983373  0.6983373  0.6983373  0.6983373 ]\n",
      " [0.73509189 0.73509189 0.73509189 0.73509189]\n",
      " [0.77378094 0.77378094 0.77378094 0.77378094]\n",
      " [0.81450625 0.81450625 0.81450625 0.81450625]\n",
      " [0.857375   0.857375   0.857375   0.857375  ]\n",
      " [0.9025     0.9025     0.9025     0.9025    ]\n",
      " [0.95       0.95       0.95       0.95      ]\n",
      " [1.         1.         1.         1.        ]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 환경 설정\n",
    "n_states = 16  # 상태 수\n",
    "n_actions = 4  # 행동 수 (상, 하, 좌, 우)\n",
    "goal_state = 15  # 목표 상태\n",
    "\n",
    "# Q-Table 초기화\n",
    "Q_table = np.zeros((n_states, n_actions))\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "learning_rate = 0.8\n",
    "discount_factor = 0.95\n",
    "exploration_prob = 0.2\n",
    "epochs = 1000\n",
    "\n",
    "# Q-Learning 알고리즘\n",
    "for epoch in range(epochs):\n",
    "    current_state = np.random.randint(0, n_states)  # 무작위 상태에서 시작\n",
    "    \n",
    "    while current_state != goal_state:\n",
    "        # 행동 선택 (epsilon-greedy 전략)\n",
    "        if np.random.rand() < exploration_prob:\n",
    "            action = np.random.randint(0, n_actions)  # 탐색\n",
    "        else:\n",
    "            action = np.argmax(Q_table[current_state])  # 활용\n",
    "\n",
    "        # 환경에서 다음 상태로 이동 (단순화된 이동)\n",
    "        next_state = (current_state + 1) % n_states\n",
    "\n",
    "        # 보상 정의 (목표 상태 도달 시 보상 부여)\n",
    "        reward = 1 if next_state == goal_state else 0\n",
    "\n",
    "        # Q-값 업데이트 (벨만 방정식 적용)\n",
    "        Q_table[current_state, action] += learning_rate * \\\n",
    "            (reward + discount_factor * np.max(Q_table[next_state]) - Q_table[current_state, action])\n",
    "        \n",
    "        current_state = next_state  # 다음 상태로 이동\n",
    "\n",
    "# 학습된 Q-Table 출력\n",
    "print(\"학습된 Q-Table:\")\n",
    "print(Q_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
